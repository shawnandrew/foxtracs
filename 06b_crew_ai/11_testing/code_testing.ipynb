{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Crewai Testing Cli feature"
      ],
      "metadata": {
        "id": "4-YacMt0QRbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GEMINI_API_KEY'] =  userdata.get('GEMINI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] =  userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "m_e7WN9q4xal"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7bvT6BGO7I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq crewai crewai-tools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLqKA-wUQVa7",
        "outputId": "acc61f5b-3f07-46f3-bed9-ee97eca842b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.2/240.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m545.9/545.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.4/211.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.6/32.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.8/76.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.1/415.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.48.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!crewai create crew pr1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgB8CGqRCMv",
        "outputId": "1ccdea6f-9445-493b-a167-e1b46e0f769e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder pr1 already exists. Do you want to override it? [y/N]: y\n",
            "\u001b[32m\u001b[1mOverriding folder pr1...\u001b[0m\n",
            "\u001b[32m\u001b[1mCreating folder pr1...\u001b[0m\n",
            "\u001b[36mSelect a provider to set up:\u001b[0m\n",
            "\u001b[36m1. openai\u001b[0m\n",
            "\u001b[36m2. anthropic\u001b[0m\n",
            "\u001b[36m3. gemini\u001b[0m\n",
            "\u001b[36m4. nvidia_nim\u001b[0m\n",
            "\u001b[36m5. groq\u001b[0m\n",
            "\u001b[36m6. ollama\u001b[0m\n",
            "\u001b[36m7. watson\u001b[0m\n",
            "\u001b[36m8. bedrock\u001b[0m\n",
            "\u001b[36m9. azure\u001b[0m\n",
            "\u001b[36m10. cerebras\u001b[0m\n",
            "\u001b[36m11. sambanova\u001b[0m\n",
            "\u001b[36m12. other\u001b[0m\n",
            "\u001b[36mq. Quit\u001b[0m\n",
            "Enter the number of your choice or 'q' to quit: 3\n",
            "\u001b[36mSelect a model to use for Gemini:\u001b[0m\n",
            "\u001b[36m1. gemini/gemini-1.5-flash\u001b[0m\n",
            "\u001b[36m2. gemini/gemini-1.5-pro\u001b[0m\n",
            "\u001b[36m3. gemini/gemini-gemma-2-9b-it\u001b[0m\n",
            "\u001b[36m4. gemini/gemini-gemma-2-27b-it\u001b[0m\n",
            "\u001b[36mq. Quit\u001b[0m\n",
            "Enter the number of your choice or 'q' to quit: 1\n",
            "Enter your GEMINI API key (press Enter to skip): paste your gemini keys here\n",
            "\u001b[32mAPI keys and model saved to .env file\u001b[0m\n",
            "\u001b[32mSelected model: gemini/gemini-1.5-flash\u001b[0m\n",
            "\u001b[32m  - Created pr1/.gitignore\u001b[0m\n",
            "\u001b[32m  - Created pr1/pyproject.toml\u001b[0m\n",
            "\u001b[32m  - Created pr1/README.md\u001b[0m\n",
            "\u001b[32m  - Created pr1/knowledge/user_preference.txt\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/__init__.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/main.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/crew.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/tools/custom_tool.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/tools/__init__.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/config/agents.yaml\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/config/tasks.yaml\u001b[0m\n",
            "\u001b[32m\u001b[1mCrew pr1 created successfully!\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pr1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9PlR_OYRLs-",
        "outputId": "9fabbcec-e912-4554-9b08-10bd25991a01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pr1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRbHzmXvRow2",
        "outputId": "aa7a961d-1cab-4664-9853-2c5ed3546f16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knowledge  pyproject.toml  README.md  src  tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change code in `src/pr1/main.py` in `test()` and `train()` functions\n",
        "#### FROM\n",
        "```python\n",
        "inputs = {\n",
        "        'topic': 'AI LLMs',\n",
        "    }\n",
        "```\n",
        "### To\n",
        "```python\n",
        "inputs = {\n",
        "        'topic': 'AI LLMs',\n",
        "        'current_year': str(datetime.now().year)\n",
        "    }\n",
        "```"
      ],
      "metadata": {
        "id": "rxdgLjOePmEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!crewai test --n_iterations 2 --model gpt-4o-mini"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cOCRbQFReJI",
        "outputId": "bef208e4-5c40-4441-83a9-224ce205e120"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the crew for 2 iterations with model gpt-4o-mini\n",
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n",
            "\u001b[1m\u001b[93m \n",
            "[2025-03-06 20:58:28][INFO]: Planning the crew execution\u001b[00m\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mConduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2025.\n",
            "1. **Information Gathering (AI LLMs Senior Data Researcher):**\n",
            "   *   **Objective:** Identify key advancements, trends, and challenges in AI LLMs as of 2025.\n",
            "   *   **Methodology:**\n",
            "        *   **Phase 1: Define Search Scope:** Determine the breadth of the research, focusing on specific areas such as model architecture, training techniques, applications, ethical considerations, and industry adoption.\n",
            "        *   **Phase 2: Keyword Identification:** Create a comprehensive list of keywords related to AI LLMs, incorporating terms like 'next-generation LLMs,' 'advanced transformer models,' 'self-supervised learning 2025,' 'AI ethics 2025,' 'LLM applications 2025,' 'fine-tuning techniques 2025,' 'federated learning LLMs,' 'explainable AI for LLMs,' and 'robustness of LLMs'.\n",
            "        *   **Phase 3: Data Source Selection:** Prioritize reputable sources, including:\n",
            "            *   Academic research papers (e.g., arXiv, IEEE Xplore, ACM Digital Library).\n",
            "            *   Industry reports and whitepapers from leading AI companies (e.g., Google AI, OpenAI, Microsoft AI).\n",
            "            *   Tech news outlets and blogs specializing in AI (e.g., VentureBeat, The AI Journal).\n",
            "            *   AI conference proceedings (e.g., NeurIPS, ICML, ICLR).\n",
            "            *   Government and regulatory publications related to AI.\n",
            "        *   **Phase 4: Data Extraction:** Extract information about:\n",
            "            *   **New Model Architectures:** Identify advancements beyond traditional transformer models (e.g., mixture of experts, sparse attention mechanisms).\n",
            "            *   **Training Techniques:** Investigate improvements in self-supervised learning, reinforcement learning, and transfer learning.\n",
            "            *   **Scalability and Efficiency:** Explore techniques for training and deploying larger LLMs with reduced computational resources.\n",
            "            *   **Applications:** Discover novel applications of LLMs across various industries (e.g., healthcare, finance, education).\n",
            "            *   **Ethical Considerations:** Analyze the ethical implications of LLMs, including bias, fairness, and transparency.\n",
            "            *   **Security and Privacy:** Examine vulnerabilities of LLMs to adversarial attacks and data breaches.\n",
            "            *   **Explainability and Interpretability:** Research methods for understanding and interpreting the decisions made by LLMs.\n",
            "            *   **Multilingual Capabilities:** Assess advancements in LLMs' ability to process and generate text in multiple languages.\n",
            "            *   **Integration with Other AI Modalities:** Investigate the integration of LLMs with computer vision, speech recognition, and robotics.\n",
            "            *   **Customization and Personalization:** Research methods for tailoring LLMs to specific users or tasks.\n",
            "        *   **Phase 5: Data Validation:** Cross-reference information from multiple sources to ensure accuracy and reliability.\n",
            "        *   **Phase 6: Information Synthesis:** Summarize the key findings and identify emerging trends.\n",
            "2. **Output Generation:**\n",
            "   *   Compile a list of 10 bullet points summarizing the most relevant information about AI LLMs in 2025. Each point should be concise, informative, and supported by evidence from the research.\n",
            "   *   Format: Each bullet point should clearly state the information, and include the source of the information\n",
            "   *   Example: 'Large Language Models are being used to generate personalized medical recommendations (Source: New England Journal of Medicine, 2025)'\n",
            "   *   Ensure the information is relevant to the context of the report.\n",
            "3. **Quality Assurance:**\n",
            "   *   Review the bullet points for accuracy, clarity, and relevance.\n",
            "   *   Ensure that the information is presented in a neutral and unbiased manner.\n",
            "   *   Verify all sources and citations.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "* **Modular LLMs are gaining traction:**  Research from NeurIPS 2024 and subsequent publications indicate a shift towards modular LLM architectures, allowing for specialized components to handle specific tasks, improving efficiency and reducing computational costs.  This modularity also enhances explainability by isolating decision-making processes within individual modules. (Source:  Composite of NeurIPS 2024 papers on modular LLMs and follow-up publications in 2025, including arXiv preprints and journals like JMLR).\n",
            "\n",
            "\n",
            "* **Reinforcement Learning from Human Feedback (RLHF) refinements are addressing alignment challenges:**  While RLHF remains a cornerstone of LLM training, 2025 saw significant advancements in mitigating issues such as reward model misspecification and the emergence of unexpected behaviors.  New techniques focusing on more robust reward functions and incorporating diverse human feedback are highlighted in papers from ICML 2025. (Source:  ICML 2025 proceedings and related publications on improved RLHF methodologies).\n",
            "\n",
            "\n",
            "* **Federated learning is enabling privacy-preserving LLM training:**  The growing emphasis on data privacy has fueled advancements in federated learning for LLMs.  Studies published in 2025 demonstrate the feasibility of training high-performing LLMs without centralizing sensitive data, improving both ethical and practical aspects of LLM development. (Source:  ACM SIGKDD Conference proceedings 2025 and research papers from Google AI and similar organizations focusing on federated learning for LLMs).\n",
            "\n",
            "\n",
            "* **Explainable AI (XAI) techniques are improving LLM transparency:**  While full transparency remains a challenge, 2025 witnessed notable progress in XAI for LLMs.  Techniques such as attention visualization, saliency mapping, and counterfactual analysis are offering greater insights into LLM decision-making processes. (Source:  AAAI Conference 2025 proceedings and papers focusing on LLM interpretability and XAI methods).\n",
            "\n",
            "\n",
            "* **LLMs are driving advancements in personalized education:**  Adaptive learning platforms leveraging LLMs are becoming increasingly sophisticated, providing personalized learning experiences tailored to individual student needs and learning styles.  Reports from educational technology companies in 2025 showcase the positive impacts on student outcomes. (Source:  Reports from companies like Khan Academy, Duolingo, and other EdTech firms detailing LLM applications in personalized education).\n",
            "\n",
            "\n",
            "* **Enhanced multilingual capabilities are bridging language barriers:**  Significant improvements in multilingual LLMs have been observed, enabling more accurate and nuanced translation and cross-lingual understanding. This facilitated better communication and information access across different linguistic groups, as evidenced by reports from organizations like the UN and the World Bank in 2025. (Source:  UN and World Bank reports on the use of multilingual LLMs and related publications in journals like Computational Linguistics).\n",
            "\n",
            "\n",
            "* **LLMs are revolutionizing drug discovery and development:**  The ability of LLMs to analyze vast datasets and predict molecular properties accelerates drug discovery processes.  Publications in high-impact journals in 2025 illustrate successful applications in identifying potential drug candidates and optimizing their design. (Source:  Nature, Science, and Cell publications featuring research using LLMs for drug discovery in 2025).\n",
            "\n",
            "\n",
            "* **Robustness against adversarial attacks is an ongoing area of focus:**  Research in 2025 continues to explore methods for strengthening LLMs against adversarial attacks, which aim to manipulate their outputs through subtle input modifications. New defensive strategies and improved detection methods are documented in various cybersecurity journals. (Source:  IEEE Security & Privacy and other cybersecurity journals publishing research on LLM robustness in 2025).\n",
            "\n",
            "\n",
            "* **Ethical considerations are driving responsible AI development:**  The potential for bias, misinformation, and misuse of LLMs has fueled the development of ethical guidelines and frameworks.  Organizations and governments actively engaged in developing ethical guidelines for AI in 2025 are regularly publishing their findings. (Source:  Reports from organizations like the OECD, EU, and various national AI ethics bodies published in 2025).\n",
            "\n",
            "\n",
            "* **LLMs are increasingly integrated with other AI modalities:**  The fusion of LLMs with computer vision, speech recognition, and robotics is leading to the development of more sophisticated and versatile AI systems.  Numerous publications in 2025 highlight the successful integration of LLMs into multi-modal applications. (Source:  Publications from leading AI conferences like CVPR, Interspeech, and IROS demonstrating multi-modal AI systems using LLMs in 2025).\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mReview the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains any and all relevant information.\n",
            "1. **Review and Organize Information (AI LLMs Reporting Analyst):**\n",
            "    *   **Objective:** Structure the information received from the AI LLMs Senior Data Researcher into a coherent report format.\n",
            "    *   **Methodology:**\n",
            "        *   **Step 1: Topic Review:** Thoroughly review the 10 bullet points provided by the researcher, understanding the nuances and implications of each point.\n",
            "        *   **Step 2: Categorization:** Group the bullet points into logical categories or themes. This could be based on:\n",
            "            *   **Model Architecture:** Advancements in transformer models.\n",
            "            *   **Training Techniques:** Improvements in learning methodologies.\n",
            "            *   **Scalability and Efficiency:** Techniques for resource optimization.\n",
            "            *   **Applications:** Industry-specific use cases.\n",
            "            *   **Ethical Considerations:** Bias, fairness, and transparency.\n",
            "            *   **Security and Privacy:** Vulnerabilities and mitigation strategies.\n",
            "            *   **Explainability and Interpretability:** Methods for understanding LLM decisions.\n",
            "            *   **Multilingual Capabilities:** Advancements in language processing.\n",
            "            *   **Integration with Other AI Modalities:** Combining LLMs with other AI technologies.\n",
            "            *   **Customization and Personalization:** Tailoring LLMs to specific needs.\n",
            "        *   **Step 3: Outlining:** Create a report outline with each category as a main section. Subsections will then be created for each related bullet point.\n",
            "2. **Content Expansion:**\n",
            "    *   **Objective:** Develop each bullet point into a comprehensive section of the report, providing detailed explanations and supporting evidence.\n",
            "    *   **Methodology:**\n",
            "        *   **Step 1: Detailed Research:** Conduct further research on each topic to gather more in-depth information. Use the same sources as the researcher (academic papers, industry reports, tech news, conference proceedings, and government publications).\n",
            "        *   **Step 2: Contextualization:** Provide context for each topic, explaining its significance and relevance to the overall field of AI LLMs.\n",
            "        *   **Step 3: Elaboration:** Expand on the information presented in the bullet points, providing additional details, examples, and case studies.\n",
            "        *   **Step 4: Visual Aids (if possible):** Include diagrams, charts, or tables to illustrate complex concepts and data (ensure these are text-based or described for accessibility, as there are no direct tools for image generation).\n",
            "        *   **Step 5: Evidence Integration:** Include references and citations to support claims and provide credibility. Ensure that all sources are properly cited.\n",
            "3. **Report Formatting:**\n",
            "    *   **Objective:** Present the information in a clear, concise, and well-organized manner.\n",
            "    *   **Methodology:**\n",
            "        *   **Step 1: Introduction:** Write an introduction that provides an overview of the report and its objectives. Mention the scope and limitations of the research.\n",
            "        *   **Step 2: Section Development:** Develop each section with a clear heading and subheadings. Use bullet points, numbered lists, and short paragraphs to improve readability.\n",
            "        *   **Step 3: Conclusion:** Summarize the key findings and provide recommendations for future research or development.\n",
            "        *   **Step 4: Formatting:** Format the report as Markdown, ensuring it is clean and professional. Avoid using '```' code blocks.\n",
            "            *   Use headers (e.g., #, ##, ###) to structure the content.\n",
            "            *   Use bullet points (*) or numbered lists (1., 2., 3.) to present information.\n",
            "            *   Use bold and italic text for emphasis.\n",
            "            *   Use links to reference external sources.\n",
            "        *   **Step 5: Review:** Review the report for grammar, spelling, and punctuation errors. Ensure that the formatting is consistent throughout the document.\n",
            "4. **Final Output:**\n",
            "    *   **Objective:** Produce a fully fleshed-out report that is detailed, informative, and well-structured.\n",
            "    *   **Format:** The final report should be in Markdown format without '```' code blocks, including:\n",
            "        *   Title\n",
            "        *   Introduction\n",
            "        *   Sections for each main topic, with detailed explanations and supporting evidence\n",
            "        *   Conclusion\n",
            "        *   References\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "# Advancements in Large Language Models (LLMs): A 2025 Report\n",
            "\n",
            "## Introduction\n",
            "\n",
            "This report summarizes key advancements in Large Language Models (LLMs) throughout 2025, drawing upon research from leading academic conferences (NeurIPS, ICML, AAAI, ACM SIGKDD, CVPR, Interspeech, IROS), high-impact journals (Nature, Science, Cell, JMLR, Computational Linguistics, IEEE Security & Privacy), and reports from prominent organizations (UN, World Bank, OECD, EU, various national AI ethics bodies, and EdTech firms like Khan Academy and Duolingo).  The report focuses on architectural improvements, training methodologies, ethical considerations, and emerging applications across various sectors.  It is important to note that this report's scope is limited to the information available from the specified sources and may not represent the entirety of LLM research in 2025.\n",
            "\n",
            "## 1. Model Architectures: The Rise of Modularity\n",
            "\n",
            "The year 2025 witnessed a significant shift towards modular LLM architectures.  Research from NeurIPS 2024 and subsequent publications (Composite of NeurIPS 2024 papers on modular LLMs and follow-up publications in 2025, including arXiv preprints and journals like JMLR) highlighted the benefits of this approach.  Modular LLMs consist of specialized components designed for specific tasks, enhancing efficiency by reducing computational costs and improving overall performance. This modularity also significantly contributes to explainability.  By isolating decision-making processes within individual modules, researchers gained greater insights into the reasoning behind LLM outputs.  This advancement facilitates debugging, allows for easier identification of biases within specific modules, and simplifies the process of adapting LLMs to new tasks or domains by adding or replacing modules as needed.  Further research is needed to standardize modularity approaches and establish best practices for module design and interaction.\n",
            "\n",
            "## 2. Training Techniques: Refining Reinforcement Learning from Human Feedback (RLHF)\n",
            "\n",
            "Reinforcement Learning from Human Feedback (RLHF) remained a central training methodology for LLMs in 2025. However,  ICML 2025 proceedings and related publications (ICML 2025 proceedings and related publications on improved RLHF methodologies) showcased substantial advancements in addressing alignment challenges.  Past issues, such as reward model misspecification leading to unpredictable behavior, were tackled through novel approaches.  These include the development of more robust reward functions that better capture desired behaviors and the incorporation of more diverse human feedback, aiming to reduce bias and improve the alignment between LLM outputs and human values.  Ongoing research focuses on creating more transparent and interpretable reward models, enabling better understanding and control over the alignment process.\n",
            "\n",
            "## 3. Scalability and Efficiency: Privacy-Preserving Training via Federated Learning\n",
            "\n",
            "Concerns about data privacy have accelerated the adoption of federated learning for LLM training.  Studies published in 2025 (ACM SIGKDD Conference proceedings 2025 and research papers from Google AI and similar organizations focusing on federated learning for LLMs) demonstrated the feasibility of training high-performing LLMs without centralizing sensitive data. This approach addresses ethical concerns and enhances the practical aspects of LLM development, especially in scenarios with stringent data protection regulations.  Federated learning allows multiple parties to collaboratively train a shared LLM without directly sharing their data, leading to better data privacy and potentially increased diversity in training datasets. However, challenges remain in terms of communication efficiency, model convergence, and ensuring fairness across participating parties.\n",
            "\n",
            "## 4. Explainability and Interpretability: Progress in Explainable AI (XAI) for LLMs\n",
            "\n",
            "While achieving full transparency in LLM decision-making remains a significant challenge, 2025 witnessed promising progress in Explainable AI (XAI) for LLMs.  AAAI Conference 2025 proceedings and related publications (AAAI Conference 2025 proceedings and papers focusing on LLM interpretability and XAI methods) highlighted various techniques, including attention visualization, saliency mapping, and counterfactual analysis, which offer improved insights into LLM internal processes.  Attention visualization helps to understand which parts of the input influenced the LLM's output, saliency mapping identifies crucial features, and counterfactual analysis explores how changes to the input would affect the output. These techniques, while still imperfect, are vital for building trust and enabling better understanding of LLM decisions, particularly in high-stakes applications such as healthcare and finance.\n",
            "\n",
            "\n",
            "## 5. Applications: Personalized Education and Beyond\n",
            "\n",
            "LLMs are rapidly transforming various sectors.  In education, adaptive learning platforms powered by LLMs (Reports from companies like Khan Academy, Duolingo, and other EdTech firms detailing LLM applications in personalized education) are increasingly prevalent, providing personalized learning experiences tailored to individual student needs and learning styles.  Reports from 2025 indicate positive impacts on student outcomes, showcasing improved engagement and learning achievements.  Beyond education, LLMs are revolutionizing other fields, as discussed in the subsequent sections.\n",
            "\n",
            "\n",
            "## 6. Multilingual Capabilities: Bridging Language Barriers\n",
            "\n",
            "Significant advancements in multilingual LLMs have facilitated better cross-lingual communication and information access.  Reports from organizations like the UN and the World Bank in 2025 (UN and World Bank reports on the use of multilingual LLMs and related publications in journals like Computational Linguistics) highlight the positive impact on global communication and collaboration.  These improved capabilities enable more accurate and nuanced translation, facilitating information exchange across different linguistic groups.  This has significant implications for international cooperation, access to information, and global understanding.  Further development is crucial to ensure equal quality of service across all languages and to address potential biases embedded in multilingual models.\n",
            "\n",
            "\n",
            "## 7. Applications: Revolutionizing Drug Discovery and Development\n",
            "\n",
            "The application of LLMs in drug discovery and development is accelerating research and innovation.  Publications in high-impact journals in 2025 (Nature, Science, and Cell publications featuring research using LLMs for drug discovery in 2025) showcase the successful use of LLMs to analyze vast datasets, predict molecular properties, and identify potential drug candidates.  LLMs can significantly reduce the time and cost associated with traditional drug discovery processes, enabling the development of new therapies for various diseases.  This is a rapidly evolving field with substantial potential to improve healthcare outcomes.\n",
            "\n",
            "\n",
            "## 8. Security and Privacy: Addressing Robustness Against Adversarial Attacks\n",
            "\n",
            "The vulnerability of LLMs to adversarial attacks remains a crucial concern.  Research in 2025 (IEEE Security & Privacy and other cybersecurity journals publishing research on LLM robustness in 2025) explored methods to enhance LLM robustness against attacks that aim to manipulate outputs through subtle input modifications.  New defensive strategies and improved detection methods were developed to mitigate these risks.  Ongoing research is essential to address the evolving nature of adversarial attacks and to develop more resilient LLM systems.  This is crucial for maintaining the trustworthiness and security of LLMs in various applications.\n",
            "\n",
            "\n",
            "## 9. Ethical Considerations: Responsible AI Development\n",
            "\n",
            "The potential for bias, misinformation, and misuse of LLMs has prompted increased focus on ethical considerations. Organizations and governments worldwide actively engaged in developing ethical guidelines for AI (Reports from organizations like the OECD, EU, and various national AI ethics bodies published in 2025) are publishing their findings regularly.  These guidelines aim to promote responsible AI development, ensuring fairness, transparency, and accountability.  Ongoing efforts are crucial to address the ethical challenges associated with LLMs and to ensure their development and deployment align with societal values and norms.\n",
            "\n",
            "\n",
            "## 10. Integration with Other AI Modalities: Towards Multi-modal AI Systems\n",
            "\n",
            "The integration of LLMs with other AI modalities, such as computer vision, speech recognition, and robotics, is creating more sophisticated and versatile AI systems.  Numerous publications in 2025 (Publications from leading AI conferences like CVPR, Interspeech, and IROS demonstrating multi-modal AI systems using LLMs in 2025) highlighted successful integrations of LLMs into multi-modal applications. This fusion of capabilities enables the creation of AI systems that can perceive, understand, and interact with the world in more comprehensive ways, paving the way for advancements in areas such as human-computer interaction, autonomous systems, and assistive technologies.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "2025 marked a year of significant progress in the development and application of LLMs.  Advancements in model architectures, training techniques, and XAI methods have improved efficiency, explainability, and robustness.  Meanwhile, the growing awareness of ethical considerations and privacy concerns is driving responsible AI development.  The integration of LLMs with other AI modalities promises even more powerful and versatile AI systems in the future. Continued research and development are crucial to address remaining challenges and fully realize the transformative potential of LLMs across diverse domains.  Further investigation into the long-term societal impacts of these powerful technologies is also necessary to ensure their beneficial and equitable deployment.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[93m \n",
            "[2025-03-06 20:59:29][INFO]: Planning the crew execution\u001b[00m\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mConduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2025.\n",
            "1. **Define Research Scope:** The AI LLMs Senior Data Researcher will begin by defining the scope of the research, focusing on advancements, applications, and challenges related to AI LLMs expected in 2025. This includes areas like model architectures, training techniques, ethical considerations, and real-world impact. \n",
            "2. **Identify Key Areas of Investigation:** Based on the research scope, identify key areas that require thorough investigation. This may include:\n",
            "    *   Advancements in Transformer architectures and novel LLM designs\n",
            "    *   Improvements in training efficiency and reduced computational costs\n",
            "    *   Applications of LLMs in specific industries (e.g., healthcare, finance, education)\n",
            "    *   Ethical considerations and bias mitigation strategies\n",
            "    *   Emerging trends in multimodal LLMs\n",
            "    *   The impact of LLMs on human-computer interaction\n",
            "    *   Regulatory landscape and governance of LLMs\n",
            "    *   Integration of LLMs with other AI technologies (e.g., reinforcement learning, computer vision)\n",
            "    *   Explainability and interpretability of LLMs\n",
            "    *   Security vulnerabilities and adversarial attacks on LLMs\n",
            "3. **Gather Information (Simulating Tool-less Research):** Since the agent has no specific tools, the researcher will meticulously gather information using general research strategies, simulating access to advanced search engines and databases:\n",
            "    *   **Simulate Advanced Search:** Imagine utilizing advanced search engine functionalities (Google Scholar with advanced filters, specialized AI research databases - assume these exist conceptually for 2025). Formulate precise search queries related to each key area identified in step 2. Example queries:\n",
            "        *   \"2025 advancements in transformer architectures\"\n",
            "        *   \"Efficient training methods for large language models 2025\"\n",
            "        *   \"Ethical challenges of LLMs 2025\"\n",
            "        *   \"Multimodal LLM applications 2025\"\n",
            "    *   **Explore Hypothetical Publications:** Envision accessing a comprehensive database of AI research papers, pre-prints, and industry reports. Identify and \"read\" (analyze conceptually) publications relevant to the research scope.\n",
            "    *   **\"Attend\" Virtual Conferences/Webinars:** Consider the hypothetical recordings or summaries of major AI conferences and webinars that occurred in 2025. Extract relevant insights and emerging trends presented by leading researchers and industry experts.\n",
            "    *   **Analyze \"Industry Reports\":** Simulate access to industry reports and market analysis documents focusing on the AI LLM landscape in 2025. Identify key market trends, adoption rates, and investment patterns.\n",
            "4. **Synthesize and Summarize Findings:**  After gathering information from various (simulated) sources, the researcher will synthesize the findings and summarize the key takeaways for each area of investigation.\n",
            "5. **Identify 10 Most Relevant Bullet Points:**  Based on the synthesized information, the researcher will identify the 10 most relevant bullet points that represent the cutting-edge developments and significant trends in AI LLMs for the year 2025. These bullet points should be concise, informative, and forward-looking.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "* **Emergence of \"Efficient Transformers\":**  Significant breakthroughs in transformer architecture have led to models with comparable performance to larger models but with drastically reduced computational requirements, enabling deployment on significantly less powerful hardware.  This includes innovations in sparsity, quantization, and novel attention mechanisms.\n",
            "\n",
            "* **Increased Focus on Multimodal LLMs:**  Multimodal LLMs that seamlessly integrate text, images, audio, and video are rapidly advancing.  These models demonstrate superior performance in tasks requiring understanding of diverse data modalities, leading to applications in augmented reality, advanced content creation, and complex robotics.\n",
            "\n",
            "* **Explainable AI (XAI) Advancements in LLMs:** While complete transparency remains a challenge, significant progress has been made in techniques to visualize and interpret the decision-making processes of LLMs, fostering greater trust and accountability.  This includes methods for identifying crucial features and visualizing attention weights.\n",
            "\n",
            "* **Enhanced Robustness and Reduced Bias:**  New training methodologies and data augmentation techniques are effectively mitigating biases in LLMs, leading to fairer and more equitable outcomes across various applications.  Research focuses on detecting and correcting biases throughout the model’s lifecycle.\n",
            "\n",
            "* **Federated Learning for LLMs:**  Federated learning is being widely adopted for training LLMs, improving data privacy and security while enabling collaborative model development across diverse datasets and organizations.\n",
            "\n",
            "* **LLMs Driving Personalized Education:**  Tailored learning experiences powered by LLMs are transforming education, providing personalized tutoring, adaptive assessments, and content generation based on individual student needs and learning styles.\n",
            "\n",
            "* **LLMs in Healthcare Revolutionizing Diagnosis and Treatment:** LLMs are increasingly used in medical image analysis, drug discovery, personalized medicine, and patient care, significantly improving efficiency and accuracy.\n",
            "\n",
            "* **Improved Security Against Adversarial Attacks:**  Advanced techniques are developed to detect and mitigate adversarial attacks targeting LLMs, enhancing their resilience against malicious manipulations aimed at compromising their integrity. This includes advanced defense mechanisms and more robust training paradigms.\n",
            "\n",
            "* **LLMs Powering Enhanced Human-Computer Interaction:**  Natural language interfaces driven by LLMs are revolutionizing human-computer interaction, creating more intuitive and efficient ways for humans to interact with complex systems and technologies.\n",
            "\n",
            "* **Regulatory Frameworks and Ethical Guidelines for LLMs:**  Governments and regulatory bodies are actively developing frameworks and guidelines to address the ethical implications of LLMs, focusing on issues like bias, transparency, accountability, and intellectual property.  These initiatives promote responsible AI development and deployment.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mReview the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains any and all relevant information.\n",
            "1. **Review the Bullet Points:** The AI LLMs Reporting Analyst will start by carefully reviewing the 10 bullet points provided by the AI LLMs Senior Data Researcher. Each bullet point represents a key topic or development in the field of AI LLMs for 2025.\n",
            "2. **Expand Each Bullet Point into a Section Outline:** For each bullet point, the analyst will create a detailed outline for a corresponding section in the report. This outline will guide the expansion of the bullet point into a comprehensive discussion. The outline should include:\n",
            "    *   **Introduction:** A brief overview of the topic and its significance.\n",
            "    *   **Detailed Explanation:** A thorough explanation of the key concepts, advancements, and challenges related to the topic. Provide specific examples, data points, and supporting evidence (derived from the researcher's findings).\n",
            "    *   **Impact and Implications:** An analysis of the potential impact and implications of the topic on various industries, society, and the future of AI.\n",
            "    *   **Future Directions:** A discussion of potential future research directions, open challenges, and emerging trends related to the topic.\n",
            "    *   **Conclusion:** A summary of the main points and key takeaways.\n",
            "3. **Develop Detailed Content for Each Section:**  Using the outline as a guide, the analyst will develop detailed content for each section. Since there are no tools, this relies on strong analytical and writing skills, and careful organization of the information presented in the bullet points.\n",
            "    *   **Elaborate on Technical Aspects:** For topics related to model architectures or training techniques, provide detailed explanations of the underlying algorithms and methodologies. Use diagrams, illustrations, and analogies to make complex concepts more accessible.\n",
            "    *   **Provide Real-World Examples:** Illustrate the practical applications of LLMs in different industries by providing real-world examples and case studies. Discuss the benefits, challenges, and lessons learned from these applications.\n",
            "    *   **Address Ethical Considerations:** For topics related to ethical considerations, discuss the potential biases, risks, and unintended consequences of LLMs. Propose mitigation strategies and best practices for responsible AI development and deployment.\n",
            "    *   **Incorporate Data and Statistics:** Support the discussion with relevant data and statistics to quantify the impact of LLMs and highlight emerging trends.\n",
            "4. **Organize the Sections into a Coherent Report:** Once the content for each section is developed, the analyst will organize the sections into a coherent report structure. Ensure that the sections flow logically and build upon each other to create a compelling narrative.\n",
            "    *   **Introduction:** Provide a brief overview of the report's purpose, scope, and key findings.\n",
            "    *   **Main Body:** Present the detailed sections on each of the 10 key topics related to AI LLMs in 2025.\n",
            "    *   **Conclusion:** Summarize the main findings of the report and highlight the key takeaways. Offer insights into the future of AI LLMs and their potential impact on society.\n",
            "5. **Format the Report as Markdown:** The analyst will format the report as markdown, ensuring readability and clarity. Use headings, subheadings, bullet points, and other formatting elements to structure the content effectively. The response must not contain any '```' characters.\n",
            "6. **Review and Edit:** The analyst will carefully review and edit the report to ensure accuracy, clarity, and completeness. Correct any grammatical errors, typos, or inconsistencies. Ensure that the report is well-organized, easy to read, and provides a comprehensive overview of the key developments in AI LLMs for 2025.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "# AI LLMs: Key Developments and Trends in 2025\n",
            "\n",
            "**Introduction:**\n",
            "\n",
            "This report analyzes ten key developments and trends shaping the landscape of Artificial Intelligence Large Language Models (LLMs) in 2025.  These advancements span architectural innovations, enhanced capabilities, ethical considerations, and regulatory frameworks, highlighting the transformative impact of LLMs across various sectors.  The report explores each topic in detail, examining its implications, future directions, and associated challenges.\n",
            "\n",
            "\n",
            "**1. Emergence of \"Efficient Transformers\"**\n",
            "\n",
            "* **Introduction:** The computational demands of LLMs have been a significant barrier to broader adoption.  \"Efficient Transformers\" represent a critical breakthrough, achieving performance comparable to larger models with significantly reduced computational resources.\n",
            "\n",
            "* **Detailed Explanation:** This advancement leverages techniques like sparsity (reducing the number of connections in the model), quantization (representing numbers with fewer bits), and novel attention mechanisms (improving efficiency of the attention process).  These methods allow deployment on less powerful hardware, such as edge devices and mobile phones, expanding the potential applications of LLMs. Examples include models using optimized attention mechanisms like Linformer or Performer, and techniques like pruning and knowledge distillation.\n",
            "\n",
            "* **Impact and Implications:**  The reduced computational cost opens doors for widespread deployment of LLMs in resource-constrained environments, facilitating real-time applications and accessibility across a broader range of devices and users.\n",
            "\n",
            "* **Future Directions:** Future research will focus on further optimizing these techniques, exploring novel architectural designs, and developing hardware specifically tailored for efficient transformer models.\n",
            "\n",
            "* **Conclusion:** Efficient Transformers are crucial for democratizing access to powerful LLMs, making them available to a wider range of developers and users.\n",
            "\n",
            "\n",
            "**2. Increased Focus on Multimodal LLMs**\n",
            "\n",
            "* **Introduction:** Multimodal LLMs represent a significant leap forward, integrating diverse data modalities like text, images, audio, and video.\n",
            "\n",
            "* **Detailed Explanation:**  These models leverage cross-modal learning to understand and generate information across different modalities, leading to superior performance in complex tasks requiring holistic understanding.  Examples include models capable of generating image descriptions from audio cues or answering questions based on images and accompanying textual information.\n",
            "\n",
            "* **Impact and Implications:**  Multimodal LLMs are transforming fields like augmented reality (AR), virtual reality (VR), content creation (generating interactive stories and videos), and robotics (developing more advanced and adaptable robots).\n",
            "\n",
            "* **Future Directions:**  Future research will focus on improving the seamless integration of various modalities, handling noisy or incomplete data, and developing more efficient training methodologies for these complex models.\n",
            "\n",
            "* **Conclusion:** Multimodal LLMs are key to creating more intelligent and versatile AI systems, with the potential to reshape multiple industries.\n",
            "\n",
            "\n",
            "**3. Explainable AI (XAI) Advancements in LLMs**\n",
            "\n",
            "* **Introduction:** The \"black box\" nature of LLMs has raised concerns about transparency and accountability.  Advances in XAI aim to make their decision-making processes more understandable.\n",
            "\n",
            "* **Detailed Explanation:**  Methods like visualizing attention weights (highlighting which parts of the input influenced the model's output) and identifying crucial features contributing to predictions enhance interpretability.  These techniques help build trust and allow for debugging and identifying potential biases.\n",
            "\n",
            "* **Impact and Implications:**  Increased transparency fosters trust in LLM-based systems, improving acceptance in high-stakes applications like healthcare and finance.  It also allows developers to identify and mitigate potential biases more effectively.\n",
            "\n",
            "* **Future Directions:**  Future research will focus on developing more robust and comprehensive XAI techniques, addressing the complexity of interactions within large models, and developing methods for explaining the reasons behind model predictions in plain language.\n",
            "\n",
            "* **Conclusion:** XAI is crucial for responsible AI development and deployment, promoting trust and accountability in LLM-based systems.\n",
            "\n",
            "\n",
            "**4. Enhanced Robustness and Reduced Bias**\n",
            "\n",
            "* **Introduction:**  Bias in LLMs is a significant concern, leading to unfair or discriminatory outcomes.  Research focuses on developing more robust and unbiased models.\n",
            "\n",
            "* **Detailed Explanation:**  New training methodologies like adversarial training (exposing the model to adversarial examples to improve its robustness) and data augmentation techniques (increasing the diversity of the training data) help mitigate bias.  Techniques like data debiasing and fairness-aware training are being actively explored.\n",
            "\n",
            "* **Impact and Implications:**  Reduced bias promotes fairer and more equitable outcomes across various applications, ensuring that LLMs are used responsibly and ethically.\n",
            "\n",
            "* **Future Directions:** Future research will delve deeper into the complexities of bias detection and mitigation, exploring techniques for identifying and correcting biases throughout the LLM's lifecycle.\n",
            "\n",
            "* **Conclusion:**  Robustness and fairness are critical for ensuring the ethical and responsible deployment of LLMs.\n",
            "\n",
            "\n",
            "**5. Federated Learning for LLMs**\n",
            "\n",
            "* **Introduction:** Federated learning enables collaborative model training without directly sharing sensitive data, addressing privacy concerns.\n",
            "\n",
            "* **Detailed Explanation:**  This approach allows multiple organizations to collaboratively train LLMs on their local datasets without exposing the data to each other or a central server. This improves data privacy and security while enabling the development of more accurate and robust models.\n",
            "\n",
            "* **Impact and Implications:**  Federated learning is revolutionizing data privacy in AI, making it possible to build powerful LLMs while adhering to strict data protection regulations.\n",
            "\n",
            "* **Future Directions:** Future research will focus on improving the efficiency and scalability of federated learning for LLMs, addressing challenges related to communication overhead and model heterogeneity.\n",
            "\n",
            "* **Conclusion:** Federated learning represents a significant advancement in responsible AI, enabling collaborative model development while protecting sensitive data.\n",
            "\n",
            "\n",
            "**6. LLMs Driving Personalized Education**\n",
            "\n",
            "* **Introduction:** LLMs are transforming education by enabling personalized learning experiences tailored to individual student needs.\n",
            "\n",
            "* **Detailed Explanation:**  LLMs can provide personalized tutoring, adaptive assessments, and content generation customized to a student's learning style and pace.  They can also offer targeted feedback and support, adapting to individual learning challenges.\n",
            "\n",
            "* **Impact and Implications:** Personalized education powered by LLMs promises to improve learning outcomes, increase engagement, and make education more accessible to diverse learners.\n",
            "\n",
            "* **Future Directions:**  Future research will focus on developing more sophisticated pedagogical models integrated with LLMs, and ensuring equitable access to these personalized learning tools.\n",
            "\n",
            "* **Conclusion:** LLMs have the potential to revolutionize education by providing personalized and adaptive learning experiences for every student.\n",
            "\n",
            "\n",
            "**7. LLMs in Healthcare Revolutionizing Diagnosis and Treatment**\n",
            "\n",
            "* **Introduction:** LLMs are significantly impacting healthcare, improving efficiency and accuracy in various tasks.\n",
            "\n",
            "* **Detailed Explanation:** LLMs are used in medical image analysis (detecting anomalies in scans), drug discovery (identifying potential drug candidates), personalized medicine (tailoring treatments to individual patients), and patient care (providing automated support and insights).\n",
            "\n",
            "* **Impact and Implications:**  Improved diagnosis and treatment significantly enhance patient outcomes and improve healthcare efficiency.\n",
            "\n",
            "* **Future Directions:**  Future research will focus on ensuring the clinical validation and regulatory compliance of LLM-based applications in healthcare, addressing safety and ethical considerations.\n",
            "\n",
            "* **Conclusion:**  LLMs are poised to transform healthcare, improving the accuracy, efficiency, and accessibility of medical care.\n",
            "\n",
            "\n",
            "**8. Improved Security Against Adversarial Attacks**\n",
            "\n",
            "* **Introduction:** LLMs are vulnerable to adversarial attacks—malicious inputs designed to deceive the model.  New techniques are enhancing their resilience.\n",
            "\n",
            "* **Detailed Explanation:**  Advanced defense mechanisms, like adversarial training and input sanitization, are being developed.  Robust training paradigms that incorporate diverse and adversarial examples improve model resilience.\n",
            "\n",
            "* **Impact and Implications:**  Enhanced security is crucial for deploying LLMs in safety-critical applications where reliability is paramount.\n",
            "\n",
            "* **Future Directions:**  Further research will focus on understanding the vulnerabilities of LLMs to emerging attack strategies, and developing more robust and adaptive defenses.\n",
            "\n",
            "* **Conclusion:**  Improving the security of LLMs is essential for building trust and ensuring their reliable deployment in critical applications.\n",
            "\n",
            "\n",
            "**9. LLMs Powering Enhanced Human-Computer Interaction**\n",
            "\n",
            "* **Introduction:** LLMs are revolutionizing how humans interact with computers, creating more natural and intuitive interfaces.\n",
            "\n",
            "* **Detailed Explanation:**  Natural language interfaces powered by LLMs allow users to interact with complex systems using everyday language.  This simplifies interaction and improves accessibility for non-technical users.\n",
            "\n",
            "* **Impact and Implications:**  Improved human-computer interaction enhances productivity, accessibility, and user experience across diverse applications.\n",
            "\n",
            "* **Future Directions:**  Future research will focus on developing more sophisticated dialogue management systems, improving natural language understanding, and creating more personalized and adaptive interfaces.\n",
            "\n",
            "* **Conclusion:**  LLMs are transforming human-computer interaction, making technology more accessible and user-friendly.\n",
            "\n",
            "\n",
            "**10. Regulatory Frameworks and Ethical Guidelines for LLMs**\n",
            "\n",
            "* **Introduction:**  The rapid development of LLMs necessitates regulatory frameworks and ethical guidelines to address potential risks and ensure responsible use.\n",
            "\n",
            "* **Detailed Explanation:**  Governments and regulatory bodies are establishing frameworks focusing on bias mitigation, transparency, accountability, and intellectual property rights.  These initiatives promote responsible AI development and deployment.\n",
            "\n",
            "* **Impact and Implications:**  Clear regulatory frameworks and ethical guidelines are crucial for fostering public trust and ensuring the ethical and responsible use of LLMs.\n",
            "\n",
            "* **Future Directions:** Future efforts will involve continuous refinement of regulations and guidelines to keep pace with the evolving capabilities of LLMs, promoting innovation while addressing potential societal risks.\n",
            "\n",
            "* **Conclusion:**  Robust regulatory frameworks and ethical guidelines are essential for ensuring the beneficial and responsible development and deployment of LLMs.\n",
            "\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "The advancements discussed in this report highlight the rapid evolution of LLMs in 2025.  From architectural innovations to ethical considerations, these developments are reshaping various sectors, creating both immense opportunities and significant challenges.  Continued research and thoughtful regulatory frameworks are essential to harness the potential of LLMs while mitigating potential risks, ensuring a future where AI benefits all of humanity.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[3m                                      Tasks Scores                                       \u001b[0m\n",
            "\u001b[3m                                 (1-10 Higher is better)                                 \u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mTasks/Crew/Agents \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mRun 1\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mRun 2\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mAvg. Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1mAgents                          \u001b[0m\u001b[1m \u001b[0m│\u001b[1m  \u001b[0m┃\n",
            "┠────────────────────┼───────┼───────┼────────────┼──────────────────────────────────┼──┨\n",
            "┃\u001b[36m \u001b[0m\u001b[36mTask 1            \u001b[0m\u001b[36m \u001b[0m│  9.5  │  9.5  │    9.5     │\u001b[32m \u001b[0m\u001b[32m- AI LLMs Senior Data Researcher\u001b[0m\u001b[32m \u001b[0m│  ┃\n",
            "┃\u001b[36m                    \u001b[0m│       │       │            │\u001b[32m \u001b[0m\u001b[32m                                \u001b[0m\u001b[32m \u001b[0m│  ┃\n",
            "┃\u001b[36m \u001b[0m\u001b[36m                  \u001b[0m\u001b[36m \u001b[0m│       │       │            │\u001b[32m \u001b[0m\u001b[32m                                \u001b[0m\u001b[32m \u001b[0m│  ┃\n",
            "┃\u001b[36m \u001b[0m\u001b[36mTask 2            \u001b[0m\u001b[36m \u001b[0m│  9.5  │  9.5  │    9.5     │\u001b[32m \u001b[0m\u001b[32m- AI LLMs Reporting Analyst     \u001b[0m\u001b[32m \u001b[0m│  ┃\n",
            "┃\u001b[36m                    \u001b[0m│       │       │            │\u001b[32m \u001b[0m\u001b[32m                                \u001b[0m\u001b[32m \u001b[0m│  ┃\n",
            "┃\u001b[36m \u001b[0m\u001b[36mCrew              \u001b[0m\u001b[36m \u001b[0m│ 9.50  │ 9.50  │    9.5     │\u001b[32m \u001b[0m\u001b[32m                                \u001b[0m\u001b[32m \u001b[0m│  ┃\n",
            "┃\u001b[36m \u001b[0m\u001b[36mExecution Time (s)\u001b[0m\u001b[36m \u001b[0m│  18   │  17   │     17     │\u001b[32m \u001b[0m\u001b[32m                                \u001b[0m\u001b[32m \u001b[0m│  ┃\n",
            "┗━━━━━━━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━┛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6t42OXUrUKwk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}