{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Crewai Enabling Planing and Planing_llm"
      ],
      "metadata": {
        "id": "4-YacMt0QRbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GEMINI_API_KEY'] =  userdata.get('GEMINI_API_KEY')\n"
      ],
      "metadata": {
        "id": "m_e7WN9q4xal"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq crewai crewai-tools\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLqKA-wUQVa7",
        "outputId": "acc61f5b-3f07-46f3-bed9-ee97eca842b4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.2/240.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m545.9/545.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.4/211.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.6/32.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.8/76.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.1/415.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.48.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!crewai create crew pr1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgB8CGqRCMv",
        "outputId": "1ccdea6f-9445-493b-a167-e1b46e0f769e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder pr1 already exists. Do you want to override it? [y/N]: y\n",
            "\u001b[32m\u001b[1mOverriding folder pr1...\u001b[0m\n",
            "\u001b[32m\u001b[1mCreating folder pr1...\u001b[0m\n",
            "\u001b[36mSelect a provider to set up:\u001b[0m\n",
            "\u001b[36m1. openai\u001b[0m\n",
            "\u001b[36m2. anthropic\u001b[0m\n",
            "\u001b[36m3. gemini\u001b[0m\n",
            "\u001b[36m4. nvidia_nim\u001b[0m\n",
            "\u001b[36m5. groq\u001b[0m\n",
            "\u001b[36m6. ollama\u001b[0m\n",
            "\u001b[36m7. watson\u001b[0m\n",
            "\u001b[36m8. bedrock\u001b[0m\n",
            "\u001b[36m9. azure\u001b[0m\n",
            "\u001b[36m10. cerebras\u001b[0m\n",
            "\u001b[36m11. sambanova\u001b[0m\n",
            "\u001b[36m12. other\u001b[0m\n",
            "\u001b[36mq. Quit\u001b[0m\n",
            "Enter the number of your choice or 'q' to quit: 3\n",
            "\u001b[36mSelect a model to use for Gemini:\u001b[0m\n",
            "\u001b[36m1. gemini/gemini-1.5-flash\u001b[0m\n",
            "\u001b[36m2. gemini/gemini-1.5-pro\u001b[0m\n",
            "\u001b[36m3. gemini/gemini-gemma-2-9b-it\u001b[0m\n",
            "\u001b[36m4. gemini/gemini-gemma-2-27b-it\u001b[0m\n",
            "\u001b[36mq. Quit\u001b[0m\n",
            "Enter the number of your choice or 'q' to quit: 1\n",
            "Enter your GEMINI API key (press Enter to skip): paste your gemini keys here\n",
            "\u001b[32mAPI keys and model saved to .env file\u001b[0m\n",
            "\u001b[32mSelected model: gemini/gemini-1.5-flash\u001b[0m\n",
            "\u001b[32m  - Created pr1/.gitignore\u001b[0m\n",
            "\u001b[32m  - Created pr1/pyproject.toml\u001b[0m\n",
            "\u001b[32m  - Created pr1/README.md\u001b[0m\n",
            "\u001b[32m  - Created pr1/knowledge/user_preference.txt\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/__init__.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/main.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/crew.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/tools/custom_tool.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/tools/__init__.py\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/config/agents.yaml\u001b[0m\n",
            "\u001b[32m  - Created pr1/src/pr1/config/tasks.yaml\u001b[0m\n",
            "\u001b[32m\u001b[1mCrew pr1 created successfully!\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/pr1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9PlR_OYRLs-",
        "outputId": "9fabbcec-e912-4554-9b08-10bd25991a01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pr1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRbHzmXvRow2",
        "outputId": "aa7a961d-1cab-4664-9853-2c5ed3546f16"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "knowledge  pyproject.toml  README.md  src  tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!crewai run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cOCRbQFReJI",
        "outputId": "d8a3b380-c8ac-46ef-8dd8-626235d8605e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Crew\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mConduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2025.\n",
            "\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "* **Multimodal LLMs are gaining significant traction:**  2025 has seen a surge in models capable of handling multiple modalities like text, images, audio, and video simultaneously.  This allows for more sophisticated and nuanced interactions, powering applications such as advanced virtual assistants, realistic video game characters, and innovative content creation tools.  Research is focusing on efficient methods for handling the vastly increased data volume and computational demands of multimodal processing.\n",
            "\n",
            "* **Improved reasoning and problem-solving capabilities:**  Significant advancements have been made in enhancing the reasoning and problem-solving abilities of LLMs.  Techniques like chain-of-thought prompting, external knowledge retrieval, and reinforcement learning from human feedback are increasingly employed to improve the accuracy and reliability of LLM outputs, particularly for complex tasks.\n",
            "\n",
            "* **Focus on explainability and interpretability:**  The \"black box\" nature of LLMs is being actively addressed.  Researchers are developing methods to understand the internal workings of LLMs better, enabling the identification of biases, errors, and the rationale behind their decisions.  This improved transparency is crucial for building trust and responsible AI systems.\n",
            "\n",
            "* **Growing concerns about bias and fairness:**  The pervasive nature of biases in training data continues to be a major concern.  Efforts are concentrating on developing robust methods for bias detection and mitigation, including data augmentation techniques, algorithmic fairness constraints, and post-processing adjustments to LLM outputs.\n",
            "\n",
            "* **Increased efficiency and reduced computational costs:**  Research is underway to create more efficient LLM architectures and training methods.  Quantization, pruning, and knowledge distillation are being actively explored to reduce the computational resources required for training and inference, making LLMs more accessible and environmentally friendly.\n",
            "\n",
            "* **Emergence of specialized LLMs:** Instead of aiming for general-purpose models, there's a growing trend towards developing specialized LLMs tailored for specific tasks or domains.  This allows for higher accuracy, efficiency, and reduced training data requirements compared to large general-purpose models. Examples include LLMs specializing in medical diagnosis, legal document analysis, or scientific research.\n",
            "\n",
            "* **Advancements in personalized LLMs:**  Research is exploring how to adapt LLMs to individual users' needs and preferences.  This includes techniques like personalized fine-tuning and prompt engineering, enabling the creation of highly customized AI assistants and learning tools.\n",
            "\n",
            "* **Enhanced security and robustness:**  The vulnerability of LLMs to adversarial attacks and malicious prompts is being addressed through techniques like adversarial training and robust optimization.  Research focuses on creating more resilient and secure LLMs that are less susceptible to manipulation and misuse.\n",
            "\n",
            "* **Integration with other AI technologies:**  LLMs are being increasingly integrated with other AI technologies, such as computer vision, robotics, and reinforcement learning, to create more complex and intelligent systems.  This integration is driving innovation in various fields, including autonomous driving, healthcare, and manufacturing.\n",
            "\n",
            "* **Ethical considerations and regulatory frameworks:**  The rapid advancement of LLMs has sparked intense debate about their ethical implications and potential societal impact.  Governments and organizations are actively developing regulatory frameworks to address issues such as bias, misinformation, job displacement, and the responsible use of AI.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mReview the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains any and all relevant information.\n",
            "\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "# Large Language Model (LLM) Landscape: A 2025 Report\n",
            "\n",
            "This report analyzes key trends and developments in the field of Large Language Models (LLMs) as of 2025.  The rapid advancement of this technology necessitates a continuous evaluation of its capabilities, limitations, and societal impact.\n",
            "\n",
            "## 1. The Rise of Multimodal LLMs\n",
            "\n",
            "2025 has witnessed a significant surge in the development and deployment of multimodal LLMs. These models transcend the limitations of unimodal architectures by processing and integrating information from multiple modalities, including text, images, audio, and video. This capability opens doors to significantly more sophisticated and nuanced interactions with AI systems.\n",
            "\n",
            "**Applications:**  Multimodal LLMs are powering a new generation of applications:\n",
            "\n",
            "* **Advanced Virtual Assistants:**  These assistants can understand and respond to complex queries involving visual and auditory input, providing a more human-like interaction experience.\n",
            "* **Realistic Video Game Characters:**  Game characters exhibit more believable and engaging behaviors due to their ability to process and react to visual and auditory cues in real-time.\n",
            "* **Innovative Content Creation Tools:**  These tools allow for the seamless generation of diverse content formats, combining text, images, and audio to create rich multimedia experiences.\n",
            "\n",
            "**Challenges:** The increased complexity of multimodal LLMs presents significant challenges:\n",
            "\n",
            "* **Data Volume:**  Training these models requires vast amounts of multimodal data, demanding significant storage and processing resources.\n",
            "* **Computational Demands:** The intricate processes involved in integrating various modalities require substantially greater computational power.\n",
            "* **Efficient Processing Methods:**  Research is heavily focused on developing efficient algorithms and architectures to mitigate the increased computational costs and improve processing speed.\n",
            "\n",
            "\n",
            "## 2. Enhanced Reasoning and Problem-Solving\n",
            "\n",
            "Significant progress has been made in enhancing the reasoning and problem-solving capabilities of LLMs.  This advancement is driven by several key techniques:\n",
            "\n",
            "* **Chain-of-Thought Prompting:**  This technique encourages LLMs to articulate their reasoning process step-by-step, improving the transparency and accuracy of their solutions.\n",
            "* **External Knowledge Retrieval:**  Integrating LLMs with external knowledge bases allows them to access and process information beyond their training data, expanding their knowledge and improving their problem-solving capabilities.\n",
            "* **Reinforcement Learning from Human Feedback (RLHF):**  This method uses human feedback to refine the LLM's responses, aligning its output with human preferences and expectations, resulting in more reliable and accurate results.  This is crucial for tasks requiring complex reasoning and decision-making.\n",
            "\n",
            "\n",
            "## 3.  Explainability and Interpretability\n",
            "\n",
            "Addressing the \"black box\" nature of LLMs is paramount.  Researchers are actively developing methods to improve the transparency and interpretability of these models.  This is crucial for:\n",
            "\n",
            "* **Bias Detection:**  Understanding the internal workings of LLMs allows for the identification and mitigation of biases present in training data and model architecture.\n",
            "* **Error Analysis:**  Analyzing the decision-making process helps pinpoint errors and improve model performance.\n",
            "* **Trust and Accountability:**  Explainable AI fosters trust and enables accountability by providing insights into the rationale behind LLM decisions.\n",
            "\n",
            "Techniques being explored include:\n",
            "\n",
            "* **Attention Mechanism Analysis:** Examining the attention weights assigned by the model to different parts of the input provides clues about the model's reasoning process.\n",
            "* **Saliency Maps:** Visualizing the input features that have the most significant impact on the model's predictions.\n",
            "* **Probing Classifiers:**  Training separate classifiers to predict internal representations within the LLM to understand the model's internal states.\n",
            "\n",
            "\n",
            "## 4. Addressing Bias and Fairness\n",
            "\n",
            "Bias in training data remains a persistent concern in the LLM field.  Efforts are concentrated on mitigating this issue through:\n",
            "\n",
            "* **Bias Detection Techniques:**  Developing methods to identify and quantify biases present in the training data and model outputs.  This includes analyzing the model's performance across different demographic groups.\n",
            "* **Data Augmentation:**  Enriching the training data with underrepresented groups and perspectives to reduce bias.\n",
            "* **Algorithmic Fairness Constraints:**  Incorporating constraints into the model training process to ensure fairness and prevent discriminatory outcomes.\n",
            "* **Post-Processing Adjustments:**  Modifying the model's outputs to mitigate biases after the model has been trained.\n",
            "\n",
            "\n",
            "## 5. Efficiency and Reduced Computational Costs\n",
            "\n",
            "The high computational demands of LLMs are driving research into efficient training methods and architectures:\n",
            "\n",
            "* **Quantization:** Reducing the precision of numerical representations to decrease memory usage and computational costs.\n",
            "* **Pruning:** Removing less important connections in the neural network to reduce its size and complexity.\n",
            "* **Knowledge Distillation:**  Training a smaller, more efficient student model to mimic the behavior of a larger, more complex teacher model.\n",
            "* **Efficient Architectures:**  Developing new architectures optimized for efficiency and reduced computational resources.\n",
            "\n",
            "These approaches are critical for making LLMs more accessible and environmentally sustainable.\n",
            "\n",
            "\n",
            "## 6. Specialization of LLMs\n",
            "\n",
            "The trend is shifting towards specialized LLMs tailored for specific tasks or domains.  This approach offers several advantages:\n",
            "\n",
            "* **Higher Accuracy:**  Models trained on specific datasets often achieve higher accuracy on their target tasks.\n",
            "* **Improved Efficiency:**  Specialized models require less computational resources than general-purpose models.\n",
            "* **Reduced Training Data:**  Smaller, specialized datasets are often sufficient for training effective models.\n",
            "\n",
            "Examples include LLMs specializing in:\n",
            "\n",
            "* **Medical Diagnosis:**  Assisting medical professionals with diagnosis and treatment planning.\n",
            "* **Legal Document Analysis:**  Analyzing legal documents and providing summaries or insights.\n",
            "* **Scientific Research:**  Assisting researchers with literature reviews and hypothesis generation.\n",
            "\n",
            "\n",
            "## 7. Personalized LLMs\n",
            "\n",
            "Adapting LLMs to individual users' needs and preferences is a growing area of research.  This involves:\n",
            "\n",
            "* **Personalized Fine-tuning:**  Fine-tuning pre-trained LLMs on individual users' data to create highly customized models.\n",
            "* **Prompt Engineering:**  Crafting prompts that elicit specific responses tailored to individual users.\n",
            "\n",
            "Personalized LLMs have the potential to revolutionize personalized learning, AI assistants, and targeted content creation.\n",
            "\n",
            "\n",
            "## 8. Enhanced Security and Robustness\n",
            "\n",
            "The vulnerability of LLMs to adversarial attacks and malicious prompts is a serious concern.  Research is focusing on:\n",
            "\n",
            "* **Adversarial Training:**  Training models on adversarial examples to make them more robust to malicious inputs.\n",
            "* **Robust Optimization:**  Developing training methods that produce models less susceptible to manipulation.\n",
            "* **Security Audits:**  Conducting thorough security audits to identify and mitigate vulnerabilities.\n",
            "\n",
            "These efforts are crucial for ensuring the safe and responsible deployment of LLMs.\n",
            "\n",
            "\n",
            "## 9. Integration with Other AI Technologies\n",
            "\n",
            "LLMs are increasingly integrated with other AI technologies, such as:\n",
            "\n",
            "* **Computer Vision:**  Combining LLMs with computer vision models for tasks requiring both textual and visual understanding.\n",
            "* **Robotics:**  Integrating LLMs into robotic systems for improved control and decision-making.\n",
            "* **Reinforcement Learning:**  Using reinforcement learning to train LLMs to perform complex tasks and interact with dynamic environments.\n",
            "\n",
            "This integration drives innovation in various fields, including autonomous driving, healthcare, and manufacturing.\n",
            "\n",
            "\n",
            "## 10. Ethical Considerations and Regulatory Frameworks\n",
            "\n",
            "The rapid development of LLMs raises significant ethical concerns:\n",
            "\n",
            "* **Bias and Discrimination:**  The risk of perpetuating and amplifying existing societal biases.\n",
            "* **Misinformation and Malicious Use:**  The potential for LLMs to be used to generate and spread false information.\n",
            "* **Job Displacement:**  Concerns about the potential impact of LLMs on employment.\n",
            "* **Privacy and Data Security:**  The need to protect user data and privacy.\n",
            "\n",
            "Governments and organizations are actively developing regulatory frameworks to address these concerns and ensure the responsible development and deployment of LLMs.  These frameworks focus on transparency, accountability, and the mitigation of potential harms.  Ongoing discussions and research are essential to establish clear guidelines and ethical standards for the field.\u001b[00m\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before enabling plann[link text](https://)ing\n",
        "\n",
        "Agents sequential Tasks\n",
        "\n",
        "```markdown\n",
        "# Agent: AI LLMs Senior Data Researcher\n",
        "## Task: Conduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2025.\n",
        "```\n",
        "\n",
        "```markdown\n",
        "# Agent: AI LLMs Reporting Analyst\n",
        "## Task: Review the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains any and all relevant information.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Jvz2OuxrIjgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enabling Planning and Planning_llm\n",
        "* File path : `/content/pr1/src/pr1/crew.py`\n",
        "\n",
        "```python\n",
        "from crewai import LLM\n",
        "planning_agent = LLM(model='gemini/gemini-2.0-flash')\n",
        "```\n",
        "\n",
        "```python\n",
        "    @crew\n",
        "\tdef crew(self) -> Crew:\n",
        "\t\t\"\"\"Creates the Pr1 crew\"\"\"\n",
        "\t\t# To learn how to add knowledge sources to your crew, check out the documentation:\n",
        "\t\t# https://docs.crewai.com/concepts/knowledge#what-is-knowledge\n",
        "\n",
        "\t\treturn Crew(\n",
        "\t\t\tagents=self.agents, # Automatically created by the @agent decorator\n",
        "\t\t\ttasks=self.tasks, # Automatically created by the @task decorator\n",
        "\t\t\tprocess=Process.sequential,\n",
        "\t\t\tverbose=True,\n",
        "      planning=True,\n",
        "      planning_llm=planning_agent\n",
        "\t\t\t# process=Process.hierarchical, # In case you wanna use that instead https://docs.crewai.com/how-to/Hierarchical/\n",
        "\t\t)\n",
        "```\n",
        "### full code:\n",
        "```python\n",
        "from crewai import Agent, Crew, Process, Task\n",
        "from crewai.project import CrewBase, agent, crew, task\n",
        "\n",
        "from crewai import LLM\n",
        "planning_agent = LLM(model='gemini/gemini-2.0-flash')\n",
        "\n",
        "# If you want to run a snippet of code before or after the crew starts,\n",
        "# you can use the @before_kickoff and @after_kickoff decorators\n",
        "# https://docs.crewai.com/concepts/crews#example-crew-class-with-decorators\n",
        "\n",
        "@CrewBase\n",
        "class Pr1():\n",
        "\t\"\"\"Pr1 crew\"\"\"\n",
        "\n",
        "\t# Learn more about YAML configuration files here:\n",
        "\t# Agents: https://docs.crewai.com/concepts/agents#yaml-configuration-recommended\n",
        "\t# Tasks: https://docs.crewai.com/concepts/tasks#yaml-configuration-recommended\n",
        "\tagents_config = 'config/agents.yaml'\n",
        "\ttasks_config = 'config/tasks.yaml'\n",
        "\n",
        "\t# If you would like to add tools to your agents, you can learn more about it here:\n",
        "\t# https://docs.crewai.com/concepts/agents#agent-tools\n",
        "\t@agent\n",
        "\tdef researcher(self) -> Agent:\n",
        "\t\treturn Agent(\n",
        "\t\t\tconfig=self.agents_config['researcher'],\n",
        "\t\t\tverbose=True\n",
        "\t\t)\n",
        "\n",
        "\t@agent\n",
        "\tdef reporting_analyst(self) -> Agent:\n",
        "\t\treturn Agent(\n",
        "\t\t\tconfig=self.agents_config['reporting_analyst'],\n",
        "\t\t\tverbose=True\n",
        "\t\t)\n",
        "\n",
        "\t# To learn more about structured task outputs,\n",
        "\t# task dependencies, and task callbacks, check out the documentation:\n",
        "\t# https://docs.crewai.com/concepts/tasks#overview-of-a-task\n",
        "\t@task\n",
        "\tdef research_task(self) -> Task:\n",
        "\t\treturn Task(\n",
        "\t\t\tconfig=self.tasks_config['research_task'],\n",
        "\t\t)\n",
        "\n",
        "\t@task\n",
        "\tdef reporting_task(self) -> Task:\n",
        "\t\treturn Task(\n",
        "\t\t\tconfig=self.tasks_config['reporting_task'],\n",
        "\t\t\toutput_file='report.md'\n",
        "\t\t)\n",
        "\n",
        "\t@crew\n",
        "\tdef crew(self) -> Crew:\n",
        "\t\t\"\"\"Creates the Pr1 crew\"\"\"\n",
        "\t\t# To learn how to add knowledge sources to your crew, check out the documentation:\n",
        "\t\t# https://docs.crewai.com/concepts/knowledge#what-is-knowledge\n",
        "\n",
        "\t\treturn Crew(\n",
        "\t\t\tagents=self.agents, # Automatically created by the @agent decorator\n",
        "\t\t\ttasks=self.tasks, # Automatically created by the @task decorator\n",
        "\t\t\tprocess=Process.sequential,\n",
        "            verbose=True,\n",
        "            planning=True,\n",
        "            planning_llm=planning_agent\n",
        "\t\t\t# process=Process.hierarchical, # In case you wanna use that instead https://docs.crewai.com/how-to/Hierarchical/\n",
        "\t\t)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "sqpti6G-I-WF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!crewai run"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pw6UWkWRhDu",
        "outputId": "0a57189b-3e96-4733-bb33-92f1206f6007"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running the Crew\n",
            "\u001b[1m\u001b[93m \n",
            "[2025-03-06 20:35:45][INFO]: Planning the crew execution\u001b[00m\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mConduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2025.\n",
            "1. **Information Gathering:** The AI LLMs Senior Data Researcher will begin by searching for information regarding AI LLMs in 2025. This will encompass advancements, trends, challenges, and breakthroughs. Sources include academic databases, industry publications, tech news outlets, AI research blogs, and conference proceedings.\n",
            "2. **Trend Identification:** The researcher will then identify key trends and innovations in LLMs expected to be prominent in 2025. Examples may include advancements in model size, training techniques, multi-modality, energy efficiency, or specific application areas.\n",
            "3. **Competitive Landscape Analysis:** The researcher will analyze the competitive landscape by identifying key players and their contributions to AI LLMs. This involves examining leading tech companies, research institutions, and open-source projects.\n",
            "4. **Ethical Considerations:** Explore ethical considerations surrounding LLMs. This includes concerns about bias, misinformation, job displacement, and responsible AI development.\n",
            "5. **Application Areas:** Examine diverse application areas of LLMs in 2025. These may include healthcare, finance, education, customer service, creative content generation, and scientific research.\n",
            "6. **Future Predictions:** Make informed predictions about the future of AI LLMs, considering factors such as technological progress, market dynamics, and societal impact.\n",
            "7. **Documentation:** Create a detailed list of the 10 most relevant bullet points about AI LLMs. This will serve as the task output for the next agent.\n",
            "8. **Review and Refine:** The researcher will review the list to ensure accuracy, relevance, and conciseness. This may involve cross-referencing information and making necessary revisions.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Senior Data Researcher\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "* **Multi-modal LLMs are the norm:**  2025 sees widespread adoption of LLMs capable of seamlessly integrating text, images, audio, and video.  Models are no longer solely text-based; they understand and generate content across multiple modalities, leading to richer and more immersive applications.\n",
            "\n",
            "* **Increased focus on efficiency and sustainability:** The immense computational resources required to train LLMs are being addressed.  Research into more efficient training algorithms, model compression techniques (like quantization and pruning), and hardware advancements (specialized AI chips) has significantly reduced the environmental footprint and cost of LLM development.\n",
            "\n",
            "* **Personalized and adaptive LLMs:**  LLMs are becoming increasingly personalized, adapting to individual user preferences and learning styles. This involves techniques like federated learning and reinforcement learning from human feedback to create tailored experiences.\n",
            "\n",
            "* **Robustness and safety advancements:** Significant progress has been made in mitigating biases, hallucinations, and adversarial attacks against LLMs.  Techniques like improved data curation, adversarial training, and explainable AI (XAI) are crucial in building more reliable and trustworthy systems.\n",
            "\n",
            "* **Explainable AI (XAI) gains traction:** The \"black box\" nature of LLMs is being challenged.  XAI methods are increasingly integrated into LLMs to provide insights into their decision-making processes, enhancing transparency and trust.\n",
            "\n",
            "* **LLMs power advancements in scientific discovery:** LLMs are accelerating scientific research by analyzing vast datasets, predicting molecular structures, generating hypotheses, and automating tedious tasks, leading to breakthroughs in various fields.\n",
            "\n",
            "* **Ethical frameworks and regulations are emerging:** Governments and organizations are developing ethical guidelines and regulations for the responsible development and deployment of LLMs, addressing concerns about bias, privacy, and misuse.\n",
            "\n",
            "* **Decentralized and open-source LLMs are flourishing:**  The rise of decentralized and open-source LLM initiatives promotes collaboration, transparency, and accessibility, fostering innovation and preventing monopolization.\n",
            "\n",
            "* **LLMs drive innovation in creative industries:** LLMs are revolutionizing creative fields like music, art, and literature, enabling new forms of expression and automating creative tasks, but also raising discussions about authorship and originality.\n",
            "\n",
            "* **Integration with other AI technologies:** LLMs are being seamlessly integrated with other AI technologies, such as computer vision, robotics, and reinforcement learning, leading to more sophisticated and versatile AI systems capable of complex tasks in diverse real-world scenarios.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mReview the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains any and all relevant information.\n",
            "1. **Context Review:** The AI LLMs Reporting Analyst will carefully review the 10 bullet points provided by the AI LLMs Senior Data Researcher.\n",
            "2. **Topic Expansion:** For each bullet point, the analyst will conduct further research to gather more detailed information. This may involve consulting additional sources, conducting literature reviews, and analyzing data.\n",
            "3. **Section Drafting:** The analyst will draft a full section for each topic, incorporating the expanded information. Each section will include an introduction, supporting details, examples, and conclusions.\n",
            "4. **Report Structuring:** The analyst will structure the report in a logical and coherent manner, with clear headings and subheadings. This ensures that the report is easy to read and understand.\n",
            "5. **Content Enrichment:** Enrich the report with relevant statistics, case studies, and real-world examples to support the key findings.\n",
            "6. **Formatting and Style:** Format the report in markdown, ensuring that it is visually appealing and easy to read. Use appropriate fonts, headings, and spacing.\n",
            "7. **Review and Editing:** The analyst will thoroughly review and edit the report to ensure accuracy, clarity, and completeness. This may involve correcting grammatical errors, clarifying ambiguous statements, and adding missing information.\n",
            "8. **Finalization:** Generate the final report, formatted as markdown, ready for distribution or presentation.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mAI LLMs Reporting Analyst\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "# AI LLMs: A 2025 Report on Trends and Transformations\n",
            "\n",
            "This report analyzes key trends shaping the landscape of Large Language Models (LLMs) in 2025, based on recent research and advancements.  We explore the evolution of LLMs across various domains, highlighting their impact on technology, science, and society.\n",
            "\n",
            "## 1. The Rise of Multi-modal LLMs\n",
            "\n",
            "**Introduction:** The era of text-only LLMs is fading.  2025 witnesses the widespread adoption of multi-modal LLMs, capable of seamlessly processing and generating content across multiple modalities, including text, images, audio, and video. This advancement significantly enhances the richness and immersiveness of LLM applications.\n",
            "\n",
            "**Supporting Details:**  Multi-modal LLMs leverage advanced architectures like transformers and convolutional neural networks (CNNs) to integrate various data types. These models learn complex relationships between different modalities, enabling them to perform tasks like image captioning, video understanding, and audio transcription with unprecedented accuracy.\n",
            "\n",
            "**Examples:**  Imagine an LLM that can analyze a medical image, generate a textual report detailing its findings, and even synthesize a patient-specific audio explanation.  Or consider an LLM creating interactive storybooks where the narrative adapts based on user choices, using images and audio to enhance the experience.\n",
            "\n",
            "**Conclusion:** The transition to multi-modal LLMs marks a crucial step towards more comprehensive and human-like AI systems.  This technology opens up a vast array of possibilities across numerous sectors, including healthcare, education, and entertainment.\n",
            "\n",
            "\n",
            "## 2.  Efficiency and Sustainability in LLM Development\n",
            "\n",
            "**Introduction:**  The substantial computational resources needed for training LLMs have spurred significant research into improving efficiency and sustainability.  Progress in training algorithms, model compression, and specialized hardware has drastically reduced the environmental impact and cost of LLM development.\n",
            "\n",
            "**Supporting Details:** Techniques like model quantization (reducing the precision of numerical representations), pruning (removing less important connections in the neural network), and knowledge distillation (training smaller \"student\" models on the knowledge of larger \"teacher\" models) have proven highly effective in reducing model size and computational requirements.  Simultaneously, advancements in hardware, such as specialized AI accelerators (e.g., GPUs and TPUs), significantly speed up training and inference.\n",
            "\n",
            "**Examples:**  Research papers showcase significant reductions in energy consumption and carbon emissions during LLM training using these optimization techniques. For instance, a study might demonstrate a 50% reduction in energy usage compared to traditional training methods.\n",
            "\n",
            "**Conclusion:**  The focus on efficiency and sustainability ensures the long-term viability of LLM development.  By reducing computational costs and environmental impact, these advancements make LLMs more accessible and encourage responsible AI innovation.\n",
            "\n",
            "\n",
            "## 3. Personalized and Adaptive LLMs: Tailoring AI to Individuals\n",
            "\n",
            "**Introduction:**  LLMs are evolving from generic tools to personalized assistants, adapting to individual user preferences and learning styles.  Federated learning and reinforcement learning from human feedback (RLHF) are key techniques driving this personalization.\n",
            "\n",
            "**Supporting Details:** Federated learning enables models to learn from decentralized data sources without directly sharing sensitive user information. RLHF allows LLMs to refine their behavior based on human feedback, aligning their responses with desired outcomes and reducing biases.\n",
            "\n",
            "**Examples:**  A personalized LLM-powered educational platform could adapt its teaching style and content based on a student's learning pace and preferred learning methods. A virtual assistant powered by an adaptive LLM could learn a user's communication style and preferences, tailoring its responses accordingly.\n",
            "\n",
            "**Conclusion:**  Personalized and adaptive LLMs hold immense potential to revolutionize various applications, fostering more engaging and effective interactions between humans and AI systems.  This personalization enhances user experience and creates more tailored and user-friendly AI applications.\n",
            "\n",
            "\n",
            "## 4. Enhancing Robustness and Safety in LLMs\n",
            "\n",
            "**Introduction:** Addressing the challenges of bias, hallucinations (generating inaccurate information), and adversarial attacks remains critical.  Significant progress is being made through improved data curation, adversarial training, and explainable AI (XAI).\n",
            "\n",
            "**Supporting Details:**  Careful data curation involves removing biased or misleading information from training datasets. Adversarial training exposes LLMs to intentionally crafted inputs designed to confuse them, making them more resilient to attacks. XAI techniques provide insights into the decision-making processes of LLMs, helping to identify and mitigate biases.\n",
            "\n",
            "**Examples:**  Research papers demonstrate how adversarial training improves the robustness of LLMs against adversarial attacks, reducing their susceptibility to manipulation. XAI methods can highlight the factors influencing an LLM's output, enabling developers to identify and address potential biases.\n",
            "\n",
            "**Conclusion:**  Improving the robustness and safety of LLMs is crucial for building trust and ensuring their responsible deployment.  These advancements are essential for mitigating potential risks and ensuring the ethical use of this powerful technology.\n",
            "\n",
            "\n",
            "## 5. The Growing Importance of Explainable AI (XAI)\n",
            "\n",
            "**Introduction:** The \"black box\" nature of LLMs is increasingly being questioned.  XAI methods are integrated to provide insights into their decision-making, enhancing transparency and fostering trust.\n",
            "\n",
            "**Supporting Details:**  XAI techniques aim to make the internal workings of LLMs more transparent.  This includes methods like attention visualization (highlighting which parts of the input influence the output), feature importance analysis, and generating textual explanations of the reasoning behind LLM decisions.\n",
            "\n",
            "**Examples:**  An XAI tool could visualize the attention weights assigned by an LLM to different words in a sentence, revealing which words played the most significant role in generating its response.  Another example is an LLM that explains its rationale for a particular decision, enhancing the user's understanding and building confidence in the model.\n",
            "\n",
            "**Conclusion:**  XAI is crucial for building trust and acceptance of LLMs.  By providing transparency and interpretability, XAI makes LLMs more accountable and facilitates their integration into critical applications where trust is paramount.\n",
            "\n",
            "\n",
            "## 6. LLMs as Catalysts for Scientific Discovery\n",
            "\n",
            "**Introduction:** LLMs are revolutionizing scientific research by analyzing massive datasets, predicting molecular structures, generating hypotheses, and automating repetitive tasks, leading to breakthroughs in various fields.\n",
            "\n",
            "**Supporting Details:**  LLMs can identify patterns in vast scientific datasets that would be impossible for humans to detect manually. They can also predict the properties of new materials or molecules, accelerating the drug discovery process and material science advancements.\n",
            "\n",
            "**Examples:**  LLMs have been used to predict the 3D structures of proteins, a crucial step in drug design.  They have also been employed to analyze genomic data, leading to discoveries in personalized medicine.\n",
            "\n",
            "**Conclusion:**  The integration of LLMs into scientific workflows is significantly accelerating the pace of discovery, enabling researchers to tackle complex problems more efficiently and effectively.  This leads to breakthroughs across diverse scientific disciplines.\n",
            "\n",
            "\n",
            "## 7. The Emergence of Ethical Frameworks and Regulations\n",
            "\n",
            "**Introduction:**  The increasing power and influence of LLMs necessitate the development of ethical guidelines and regulations to address concerns about bias, privacy, and misuse.\n",
            "\n",
            "**Supporting Details:**  Governments and organizations are developing frameworks to ensure fairness, transparency, and accountability in the development and deployment of LLMs.  These efforts focus on mitigating biases in training data, protecting user privacy, and preventing the misuse of LLMs for malicious purposes.\n",
            "\n",
            "**Examples:**  Some governments are exploring regulations on the use of LLMs in sensitive applications, such as hiring and loan applications.  Organizations are developing ethical guidelines for LLM developers, promoting responsible AI practices.\n",
            "\n",
            "**Conclusion:**  The development of robust ethical frameworks and regulations is crucial for ensuring the responsible and beneficial use of LLMs, minimizing potential harms, and fostering public trust.\n",
            "\n",
            "\n",
            "## 8. The Flourishing Decentralized and Open-Source LLM Ecosystem\n",
            "\n",
            "**Introduction:**  The rise of decentralized and open-source LLM initiatives promotes collaboration, transparency, and accessibility, fostering innovation and preventing monopolization.\n",
            "\n",
            "**Supporting Details:**  Open-source LLMs allow researchers and developers to collaborate on improving models, promoting transparency and reducing reliance on a few dominant players.  Decentralized platforms enable wider participation and reduce the risk of centralized control over this powerful technology.\n",
            "\n",
            "**Examples:**  Several open-source projects are developing and releasing LLMs, enabling researchers and developers to contribute to their improvement and adaptation.  Decentralized platforms provide mechanisms for community governance and oversight.\n",
            "\n",
            "**Conclusion:**  The open-source and decentralized nature of these initiatives ensures broader access, promotes collaboration, and mitigates the risks associated with centralized control over powerful AI technologies.\n",
            "\n",
            "\n",
            "## 9. LLMs and the Creative Industries: Transformation and Challenges\n",
            "\n",
            "**Introduction:** LLMs are transforming creative industries like music, art, and literature, enabling new forms of expression and automating creative tasks.  However, this transformation raises discussions about authorship and originality.\n",
            "\n",
            "**Supporting Details:**  LLMs can assist artists in generating new ideas, composing music, creating artwork, and writing stories.  They can automate time-consuming tasks, freeing up creators to focus on more conceptual aspects of their work.  However, questions remain about the nature of creativity and the attribution of authorship when LLMs are involved.\n",
            "\n",
            "**Examples:**  LLMs are being used to generate novel musical pieces, create unique artwork styles, and assist writers in crafting compelling narratives.  However, debates continue regarding copyright and ownership of works generated with the assistance of LLMs.\n",
            "\n",
            "**Conclusion:**  LLMs offer incredible potential for creative expression and innovation but also raise important ethical and legal considerations that need to be addressed to ensure responsible use and equitable outcomes.\n",
            "\n",
            "\n",
            "## 10. LLMs: Integration with Other AI Technologies\n",
            "\n",
            "**Introduction:**  LLMs are increasingly integrated with other AI technologies, such as computer vision, robotics, and reinforcement learning, resulting in more sophisticated and versatile AI systems capable of tackling complex tasks.\n",
            "\n",
            "**Supporting Details:**  Combining LLMs with computer vision allows AI systems to interpret visual information and interact with the world through language.  Integrating LLMs with robotics creates intelligent robots capable of understanding and responding to complex commands.  Reinforcement learning enables LLMs to learn and improve their performance through trial and error.\n",
            "\n",
            "**Examples:**  Imagine a robot powered by an LLM that can understand and respond to verbal instructions to perform complex tasks.  Or consider an AI system that can analyze images, generate a textual description, and then answer questions about the image's content.\n",
            "\n",
            "**Conclusion:** The synergy between LLMs and other AI technologies unlocks new possibilities for creating highly intelligent and capable AI systems with wide-ranging applications across diverse domains.  This integration marks a significant step towards building truly versatile and intelligent artificial systems.\u001b[00m\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After enabling planning\n",
        "\n",
        "Agents sequential Tasks\n",
        "\n",
        "```markdown\n",
        "[2025-03-06 20:35:45][INFO]: Planning the crew execution\n",
        "# Agent: AI LLMs Senior Data Researcher\n",
        "## Task: Conduct a thorough research about AI LLMs Make sure you find any interesting and relevant information given the current year is 2025.\n",
        "1. **Information Gathering:** The AI LLMs Senior Data Researcher will begin by searching for information regarding AI LLMs in 2025. This will encompass advancements, trends, challenges, and breakthroughs. Sources include academic databases, industry publications, tech news outlets, AI research blogs, and conference proceedings.\n",
        "2. **Trend Identification:** The researcher will then identify key trends and innovations in LLMs expected to be prominent in 2025. Examples may include advancements in model size, training techniques, multi-modality, energy efficiency, or specific application areas.\n",
        "3. **Competitive Landscape Analysis:** The researcher will analyze the competitive landscape by identifying key players and their contributions to AI LLMs. This involves examining leading tech companies, research institutions, and open-source projects.\n",
        "4. **Ethical Considerations:** Explore ethical considerations surrounding LLMs. This includes concerns about bias, misinformation, job displacement, and responsible AI development.\n",
        "5. **Application Areas:** Examine diverse application areas of LLMs in 2025. These may include healthcare, finance, education, customer service, creative content generation, and scientific research.\n",
        "6. **Future Predictions:** Make informed predictions about the future of AI LLMs, considering factors such as technological progress, market dynamics, and societal impact.\n",
        "7. **Documentation:** Create a detailed list of the 10 most relevant bullet points about AI LLMs. This will serve as the task output for the next agent.\n",
        "8. **Review and Refine:** The researcher will review the list to ensure accuracy, relevance, and conciseness. This may involve cross-referencing information and making necessary revisions.\n",
        "\n",
        "```\n",
        "\n",
        "```markdown\n",
        "# Agent: AI LLMs Reporting Analyst\n",
        "## Task: Review the context you got and expand each topic into a full section for a report. Make sure the report is detailed and contains any and all relevant information.\n",
        "1. **Context Review:** The AI LLMs Reporting Analyst will carefully review the 10 bullet points provided by the AI LLMs Senior Data Researcher.\n",
        "2. **Topic Expansion:** For each bullet point, the analyst will conduct further research to gather more detailed information. This may involve consulting additional sources, conducting literature reviews, and analyzing data.\n",
        "3. **Section Drafting:** The analyst will draft a full section for each topic, incorporating the expanded information. Each section will include an introduction, supporting details, examples, and conclusions.\n",
        "4. **Report Structuring:** The analyst will structure the report in a logical and coherent manner, with clear headings and subheadings. This ensures that the report is easy to read and understand.\n",
        "5. **Content Enrichment:** Enrich the report with relevant statistics, case studies, and real-world examples to support the key findings.\n",
        "6. **Formatting and Style:** Format the report in markdown, ensuring that it is visually appealing and easy to read. Use appropriate fonts, headings, and spacing.\n",
        "7. **Review and Editing:** The analyst will thoroughly review and edit the report to ensure accuracy, clarity, and completeness. This may involve correcting grammatical errors, clarifying ambiguous statements, and adding missing information.\n",
        "8. **Finalization:** Generate the final report, formatted as markdown, ready for distribution or presentation.\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "eWxxBOF0LBQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6t42OXUrUKwk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}