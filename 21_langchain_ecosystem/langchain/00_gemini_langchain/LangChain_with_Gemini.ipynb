{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts9Ro5dgQevN",
        "outputId": "15b670fb-f286-4262-dd05-979f50d3e983"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OaFyyFGonc3",
        "outputId": "98eb6c0b-10e6-4066-fac7-de2c4c63cf39"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Json file for Auth or allowing Gemini api in Langchain\n",
        "Adding Api key to environment variables, and getting it's value sometime cause auth issues, In case of facing Auth Issues, Follow the following steps if you are getting Auth related errors in getting response from Gemini, and being asked for auth.\n",
        "  - Open [Google Cloud Console](https://console.cloud.google.com/), at the top left corner, click on **Select a Project**, a new Window screen will pop up, Select an existing one or create a new Project. (Free Version).\n",
        "  - After Selecting or Creating a project, below **WELCOME** screen, Select **APIs and Services** from **Quick Access**.\n",
        "  - On *APIs and Services* windows, look for Library and left Sidebar. Select **Library**.\n",
        "  - In search box, type **Gemini Api**, two results will be shown,\n",
        "      1. *Gemini Api*\n",
        "      2. *Gemini for google cloud*\n",
        "\n",
        "  Select first one. Click on **Enable**. Gemini Api will be enabled.\n",
        "\n",
        "  - Now get back to **APIs and and Services** Window.\n",
        "  - Select **Credentials**, at Top level, after Credentials, Select **Create Credentail**, and click on API Key from the new dropdown. It will generate an api key.\n",
        "  - Click on **Google Cloud**, at top left corner. It will take you to Welcome page.\n",
        "  - From **Quick Access**, Select **IAM and Admin**.\n",
        "  - At left sidebar select **Service accounts** and click on **Create Service Account**\n",
        "      - Fill in the required Details, click on **Create and Continue**.\n",
        "      - Click on **Select Role**, search of *OWNER*, and select the one with full previlages.\n",
        "      - Click on **Continue**.\n",
        "      - Skip the fields, click on **Done**.\n",
        "      - At *Service Accounts* windows, click on the Email, we just created.\n",
        "      - Click the Newly generated Email.\n",
        "      - At the top, below the Key name where it's mentioned, Click on **KEYS**, from the taskbar above,\n",
        "      - At **Keys** window, select **Add Key**.\n",
        "          - Click on **Create new Key**.\n",
        "          - Select **Add Key**.\n",
        "          - Make sure **JSON** is selected.\n",
        "          - Click on **Create Key**. A New will be popup, and ask you to save the file locally.\n",
        "          - Save the json file, and upload it to the root directory of Colab.\n",
        "          - Right click on the newly add file, select **Copy Path**. Replace the path in below cell."
      ],
      "metadata": {
        "id": "5o97VoIzdXGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/fresh-key.json\""
      ],
      "metadata": {
        "id": "gZnwV44lr7MS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "ai_msg = llm.invoke(\"What is the capital of France?\")\n",
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "AW_TzN5-QhoO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a11d6b1-41a1-4c7a-9a48-73e3aa83bbd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is **Paris**. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Which open source AI Model is best so far\"},\n",
        "]\n",
        "\n",
        "ai_msg = llm.invoke(message)"
      ],
      "metadata": {
        "id": "JA4RDvoNkmlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "def to_markdown(text)->Markdown:\n",
        "    text : str = text.replace(\"•\", \"  *\")\n",
        "    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "UQvFa8xH1eKN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_markdown(ai_msg.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "zZ6t1eJO1j1F",
        "outputId": "ec564e47-b71b-4529-e37e-d07f6c97a051"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> It's impossible to definitively say which open-source AI model is \"best\" because the ideal model depends on your specific needs and goals. Here's a breakdown of some popular open-source AI models and their strengths:\n> \n> **Large Language Models (LLMs)**\n> \n> * **BLOOM (BigScience Large Open-science Open-access Multilingual Language Model):** A multilingual model trained on massive text datasets in 46 languages. Excellent for tasks like translation, text summarization, and code generation.\n> * **GPT-Neo (EleutherAI):** A family of models ranging in size, with GPT-Neo 2.7B and GPT-Neo 1.3B being popular choices. Offers strong performance in text generation, language understanding, and code completion.\n> * **GPT-J (EleutherAI):** A 6B parameter model known for its impressive text generation capabilities. \n> * **Flan-T5 (Google):** A model based on the T5 architecture, fine-tuned on a massive dataset of text and code. Excellent for code generation, question answering, and summarization.\n> * **BigScience T0:** A powerful model specifically designed for multilingual tasks. It can understand and generate text in many languages, making it ideal for cross-lingual tasks.\n> \n> **Other Open-Source Models**\n> \n> * **Stable Diffusion (Stability AI):** A powerful text-to-image generation model known for its high-quality and creative outputs.\n> * **Whisper (OpenAI):** A speech recognition model trained on a massive dataset of audio and text. Excellent for transcribing audio files and understanding spoken language.\n> * **DeepSpeech (Mozilla):** Another popular open-source speech recognition model.\n> * **YOLO (You Only Look Once):** An object detection model known for its speed and accuracy.\n> \n> **Factors to Consider when Choosing:**\n> \n> * **Task:** What specific tasks do you need the model for (e.g., text generation, translation, image generation, speech recognition)?\n> * **Model Size:** Larger models generally offer better performance but require more computational resources.\n> * **Language Support:** If you need multilingual support, consider models like BLOOM or BigScience T0.\n> * **Availability of Resources:** Make sure you have the necessary hardware and software to run the chosen model.\n> * **Community and Support:** Choose a model with a strong community and active development.\n> \n> **Recommendations:**\n> \n> * **For general text generation and language understanding:** GPT-Neo, GPT-J, or BLOOM.\n> * **For multilingual tasks:** BLOOM or BigScience T0.\n> * **For code generation:** Flan-T5.\n> * **For image generation:** Stable Diffusion.\n> * **For speech recognition:** Whisper or DeepSpeech.\n> \n> Remember to explore the specific capabilities and limitations of each model before making a decision. \n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}